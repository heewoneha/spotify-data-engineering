{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63472f3-c33f-4ea1-ae48-bdef7999c077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import os\n",
    "\n",
    "\n",
    "class BlobUploader:\n",
    "    def __init__(self, title, account_name, account_key, container_name):\n",
    "      self.account_name = account_name\n",
    "      self.account_key = account_key\n",
    "      self.container_name = container_name\n",
    "      self.title = title\n",
    "    \n",
    "    def save_parquet_to_blob(self, df):\n",
    "      parquet_path = f\"/mnt/blobstorage/{self.title}\"\n",
    "      df.write.mode('overwrite').parquet(parquet_path)\n",
    "    \n",
    "    def delete_trash_files(self):\n",
    "      directory_path = f\"/mnt/blobstorage/{self.title}\"\n",
    "      files = dbutils.fs.ls(directory_path)\n",
    "      prefixes_to_delete = ['_committed_', '_started_', '_SUCCESS']\n",
    "      \n",
    "      for file_info in files:\n",
    "          file_path = file_info.path\n",
    "          if any(file_path.startswith('dbfs:'+directory_path+'/'+prefix) for prefix in prefixes_to_delete):\n",
    "              dbutils.fs.rm(file_path)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Pyspark preprocessing') \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "\n",
    "AZURE_BLOB_ACCOUNT_NAME = os.getenv('AZURE_BLOB_ACCOUNT_NAME')\n",
    "AZURE_BLOB_ACCOUNT_KEY = os.getenv('AZURE_BLOB_ACCOUNT_KEY')\n",
    "AZURE_BLOB_CONTAINER_NAME = os.getenv('AZURE_BLOB_CONTAINER_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0509780-514e-4dd3-a661-03b075403cdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_df_from_catalog(table_name):\n",
    "  driver = \"org.postgresql.Driver\"\n",
    "  database_host = os.getenv('SERVER_NAME')\n",
    "  database_name = os.getenv('DATABASE_NAME')\n",
    "  schema = os.getenv('SCHEMA_NAME')\n",
    "  user = os.getenv('ADMIN_USER_NAME')\n",
    "  password = os.getenv('DB_PASSWORD')\n",
    "  database_port = '5432'\n",
    "  url = f\"jdbc:postgresql://{database_host}:{database_port}/{database_name}\"\n",
    "  \n",
    "  df = (spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"driver\", driver)\n",
    "        .option(\"url\", url)\n",
    "        .option(\"dbtable\", f'{schema}.{table_name}')\n",
    "        .option(\"user\", user)\n",
    "        .option(\"password\", password)\n",
    "        .load()\n",
    "        )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de09216d-837e-4439-8546-31222bbce7f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_df(title, df):\n",
    "    blob = BlobUploader(\n",
    "        title,\n",
    "        AZURE_BLOB_ACCOUNT_NAME,\n",
    "        AZURE_BLOB_ACCOUNT_KEY,\n",
    "        AZURE_BLOB_CONTAINER_NAME\n",
    "    )\n",
    "\n",
    "    dbutils.fs.mount(\n",
    "        source = f\"wasbs://{blob.container_name}@{blob.account_name}.blob.core.windows.net/\",\n",
    "        mount_point = \"/mnt/blobstorage\",\n",
    "        extra_configs = {\"fs.azure.account.key.\" + blob.account_name + \".blob.core.windows.net\": blob.account_key}\n",
    "    )\n",
    "\n",
    "    blob.save_parquet_to_blob(df)\n",
    "    blob.delete_trash_files()\n",
    "\n",
    "    dbutils.fs.unmount(\"/mnt/blobstorage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3e6bd4-e86c-496b-9e48-c3353bebd19a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# top 50 tracks info\n",
    "top50_tracks_info_df = create_df_from_catalog('top50_tracks_info')\n",
    "title = 'preprocessed_top50_tracks_info'\n",
    "save_df(title, top50_tracks_info_df)\n",
    "\n",
    "# top 50 tracks features\n",
    "top50_audio_features_df = create_df_from_catalog('top50_audio_features')\n",
    "title = 'preprocessed_top50_audio_features'\n",
    "save_df(title, top50_audio_features_df)\n",
    "\n",
    "# group artist info\n",
    "boy_group_info_df = create_df_from_catalog('kpop_boy_group_artist_info')\n",
    "girl_group_info_df = create_df_from_catalog('kpop_girl_group_artist_info')\n",
    "\n",
    "boy_group_info_with_gender_df = boy_group_info_df.withColumn('gender', lit(0))\n",
    "girl_group_info_with_gender_df = girl_group_info_df.withColumn('gender', lit(1))\n",
    "\n",
    "group_info_df = boy_group_info_with_gender_df.unionAll(girl_group_info_with_gender_df)\n",
    "title = 'preprocessed_group_artist_info'\n",
    "save_df(title, group_info_df)\n",
    "\n",
    "# group track info\n",
    "boy_group_track_info_df = create_df_from_catalog('kpop_boy_group_track_info')\n",
    "girl_group_track_info_df = create_df_from_catalog('kpop_girl_group_track_info')\n",
    "\n",
    "group_track_info_df = boy_group_track_info_df.unionAll(girl_group_track_info_df)\n",
    "title = 'preprocessed_group_track_info'\n",
    "save_df(title, group_track_info_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocesseing_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
